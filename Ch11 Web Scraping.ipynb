{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: mapIt.py with the webbrowser Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "webbrowser.open('URL網頁地址') 打開瀏覽器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('http://inventwithpython.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下 mapIt.py 需要在command line中執行，檔案資料夾要設定在系統PATH當中才可以直接執行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sys.argv是紀錄輸入command line的字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! python3\n",
    "# mapIt.py - Launches a map in the browser using an address from the command line or clipboard.\n",
    "\n",
    "import webbrowser, sys, pyperclip\n",
    "\n",
    "#sys.argv是輸入cmd的指令list\n",
    "if len(sys.argv) > 1:\n",
    "    # Get address from command line.\n",
    "    address = ' '.join(sys.argv[1:])\n",
    "\n",
    "else:\n",
    "    # Get address from clipboard.\n",
    "    address = pyperclip.paste()\n",
    "\n",
    "#Launch browser\n",
    "webbrowser.open('https://www.google.com/maps/place/' + address)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在command line中輸入: mapIt Taiwan，即可自動開啟瀏覽器查詢Taiwan地址"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Files from the Web with the requests Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests.get('URL') 可以下載該檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt')\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code == requests.codes.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178981"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Proje\n"
     ]
    }
   ],
   "source": [
    "print(res.text[:250]) #列出前250個字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res.raise_for_status() 可以看下載狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a973a2cccd39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://inventwithpython.com/page_that_does_not_exist'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 928\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://inventwithpython.com/page_that_does_not_exist')\n",
    "res.raise_for_status() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('http://inventwithpython.com/page_that_does_not_exist')\n",
    "\n",
    "#使用try/except使程式執行\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Downloaded Files to the Hard Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iter_content() method 回傳 “chunks”(區塊) of the content on eachiteration through the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt')\n",
    "res.raise_for_status()\n",
    "\n",
    "playFile = open('RomeoAndJuliet.txt', 'wb') #write binary mode.\n",
    "for chunk in res.iter_content(100000):\n",
    "    playFile.write(chunk)\n",
    "playFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Call requests.get() to download the file.\n",
    "2. Call open() with 'wb' to create a new file in write binary mode.\n",
    "3. Loop over the Response object’s iter_content() method.\n",
    "4. Call write() on each iteration to write the content to the file.\n",
    "5. Call close() to close the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML with the BeautifulSoup Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要指定 bs4.BeautifulSoup(res.text, \"html.parser\")後面的那個語法，才不會Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "res = requests.get('http://nostarch.com')\n",
    "res.raise_for_status()\n",
    "noStarchSoup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "type(noStarchSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleFile = open('example.html')\n",
    "exampleSoup = bs4.BeautifulSoup(exampleFile, \"html.parser\") #不打後面會有warning\n",
    "type(exampleSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select() Method 尋找其中element"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup.select('div') \n",
    "    All elements named <div>\n",
    "soup.select('#author') \n",
    "    The element with an id attribute of author\n",
    "soup.select('.notice') \n",
    "    All elements that use a CSS class attribute named notice\n",
    "soup.select('div span') \n",
    "    All elements named <span> that are within an element named <div>\n",
    "soup.select('div > span') \n",
    "    All elements named <span> that are directly within an element named <div>, with no other element in between\n",
    "soup.select('input[name]') \n",
    "    All elements named <input> that have a name attribute with any value\n",
    "soup.select('input[type=\"button\"]') \n",
    "    All elements named <input> that have an attribute named type with value button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "exampleFile = open('example.html')\n",
    "exampleSoup = bs4.BeautifulSoup(exampleFile.read(), \"html.parser\")\n",
    "\n",
    "#尋找標籤中有author的一段\n",
    "elems = exampleSoup.select('#author') \n",
    "\n",
    "type(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(elems[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".getText()獲取標籤中間的字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al Sweigart'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .getText()獲取標籤中間的字串\n",
    "elems[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span id=\"author\">Al Sweigart</span>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(elems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'author'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Download my <strong>Python</strong> book from <a href=\"http://\\ninventwithpython.com\">my website</a>.</p>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尋找所有標籤有<p>的\n",
    "pElems = exampleSoup.select('p')\n",
    "str(pElems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Download my Python book from my website.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .getText()獲取標籤中間的字串\n",
    "pElems[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p class=\"slogan\">Learn Python the easy way!</p>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(pElems[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learn Python the easy way!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pElems[1].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>By <span id=\"author\">Al Sweigart</span></p>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(pElems[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By Al Sweigart'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pElems[2].getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Data from an Element’s Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get() method for Tag objects makes it simple to access attribute values from an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span id=\"author\">Al Sweigart</span>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "soup = bs4.BeautifulSoup(open('example.html'),\"html.parser\")\n",
    "spanElem = soup.select('span')[0]\n",
    "str(spanElem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'author'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanElem.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanElem.get('some_nonexistent_addr') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'author'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanElem.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: “I’m Feeling Lucky” Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googling...\n"
     ]
    }
   ],
   "source": [
    "# 這個程式要command line直接開\n",
    "\n",
    "#! python3\n",
    "# lucky.py - Opens several Google search results.\n",
    "\n",
    "import requests, sys, webbrowser, bs4\n",
    "print('Googling...') # display text while downloading the Google page\n",
    "res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))\n",
    "res.raise_for_status()\n",
    "\n",
    "# Retrieve top search result links.\n",
    "soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# Open a browser tab for each result.\n",
    "# the selector '.r a' to find all <a> elements that are within an element that has the r CSS class.\n",
    "linkElems = soup.select('.r a') \n",
    "numOpen = min(5, len(linkElems))\n",
    "for i in range(numOpen):\n",
    "    webbrowser.open('http://google.com' + linkElems[i].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Downloading All XKCD Comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page http://xkcd.com...\n",
      "Downloading image //imgs.xkcd.com/comics/communicating.png...\n",
      "Downloading page http://xkcd.com/1859/...\n",
      "Downloading image //imgs.xkcd.com/comics/sports_knowledge.png...\n",
      "Downloading page http://xkcd.com/1858/...\n",
      "Downloading image //imgs.xkcd.com/comics/4th_of_july.png...\n",
      "Downloading page http://xkcd.com/1857/...\n",
      "Downloading image //imgs.xkcd.com/comics/emoji_movie.png...\n",
      "Downloading page http://xkcd.com/1856/...\n",
      "Downloading image //imgs.xkcd.com/comics/existence_proof.png...\n",
      "Downloading page http://xkcd.com/1855/...\n",
      "Downloading image //imgs.xkcd.com/comics/telephoto.png...\n",
      "Downloading page http://xkcd.com/1854/...\n",
      "Downloading image //imgs.xkcd.com/comics/refresh_types.png...\n",
      "Downloading page http://xkcd.com/1853/...\n",
      "Downloading image //imgs.xkcd.com/comics/once_per_day.png...\n",
      "Downloading page http://xkcd.com/1852/...\n",
      "Downloading image //imgs.xkcd.com/comics/election_map.png...\n",
      "Downloading page http://xkcd.com/1851/...\n",
      "Downloading image //imgs.xkcd.com/comics/magnetohydrodynamics.png...\n",
      "Downloading page http://xkcd.com/1850/...\n",
      "Downloading image //imgs.xkcd.com/comics/air_force_museum.png...\n",
      "Downloading page http://xkcd.com/1849/...\n",
      "Downloading image //imgs.xkcd.com/comics/decades.png...\n",
      "Downloading page http://xkcd.com/1848/...\n",
      "Downloading image //imgs.xkcd.com/comics/glacial_erratic.png...\n",
      "Downloading page http://xkcd.com/1847/...\n",
      "Downloading image //imgs.xkcd.com/comics/dubious_study.png...\n",
      "Downloading page http://xkcd.com/1846/...\n",
      "Downloading image //imgs.xkcd.com/comics/drone_problems.png...\n",
      "Downloading page http://xkcd.com/1845/...\n",
      "Downloading image //imgs.xkcd.com/comics/state_word_map.png...\n",
      "Downloading page http://xkcd.com/1844/...\n",
      "Downloading image //imgs.xkcd.com/comics/voting_systems.png...\n",
      "Downloading page http://xkcd.com/1843/...\n",
      "Downloading image //imgs.xkcd.com/comics/opening_crawl.png...\n",
      "Downloading page http://xkcd.com/1842/...\n",
      "Downloading image //imgs.xkcd.com/comics/anti_drone_eagles.png...\n",
      "Downloading page http://xkcd.com/1841/...\n",
      "Downloading image //imgs.xkcd.com/comics/who.png...\n",
      "Downloading page http://xkcd.com/1840/...\n",
      "Downloading image //imgs.xkcd.com/comics/genetic_testing_results.png...\n",
      "Downloading page http://xkcd.com/1839/...\n",
      "Downloading image //imgs.xkcd.com/comics/doctor_visit.png...\n",
      "Downloading page http://xkcd.com/1838/...\n",
      "Downloading image //imgs.xkcd.com/comics/machine_learning.png...\n",
      "Downloading page http://xkcd.com/1837/...\n",
      "Downloading image //imgs.xkcd.com/comics/rental_car.png...\n",
      "Downloading page http://xkcd.com/1836/...\n",
      "Downloading image //imgs.xkcd.com/comics/okeanos.png...\n",
      "Downloading page http://xkcd.com/1835/...\n",
      "Downloading image //imgs.xkcd.com/comics/random_obsessions.png...\n",
      "Downloading page http://xkcd.com/1834/...\n",
      "Downloading image //imgs.xkcd.com/comics/lunch_order.png...\n"
     ]
    }
   ],
   "source": [
    "#! python3\n",
    "# downloadXkcd.py - Downloads every single XKCD comic.\n",
    "\n",
    "import requests, os, bs4\n",
    "\n",
    "url = 'http://xkcd.com' # starting url\n",
    "os.makedirs('xkcd', exist_ok=True) # store comics in ./xkcd\n",
    "\n",
    "while not url.endswith('#'):\n",
    "    \n",
    "    # Download the page.\n",
    "    print('Downloading page %s...' % url)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # Find the URL of the comic image.\n",
    "    comicElem = soup.select('#comic img')\n",
    "    \n",
    "    if comicElem == []:\n",
    "        print('Could not find comic image.')\n",
    "    else:\n",
    "        comicUrl = comicElem[0].get('src')\n",
    "        # Download the image.\n",
    "        print('Downloading image %s...' % (comicUrl))\n",
    "        res = requests.get(\"http:\" + comicUrl)\n",
    "        res.raise_for_status()\n",
    "    \n",
    "        # Save the image to ./xkcd.\n",
    "        imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')\n",
    "        for chunk in res.iter_content(100000):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "    \n",
    "    # Get the Prev button's url.\n",
    "    prevLink = soup.select('a[rel=\"prev\"]')[0]\n",
    "    url = 'http://xkcd.com' + prevLink.get('href')\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Controlling the Browser with the selenium Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先安裝selenium到conda，然後安裝Firefox第三方軟體geckodriver到PATH當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.firefox.webdriver.WebDriver"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox()\n",
    "type(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser.get('http://inventwithpython.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Elements on the Page"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "browser.find_element_by_class_name(name)\n",
    "browser.find_elements_by_class_name(name)\n",
    "    Elements that use the CSS class name\n",
    "\n",
    "browser.find_element_by_css_selector(selector)\n",
    "browser.find_elements_by_css_selector(selector)\n",
    "    Elements that match the CSS selector\n",
    "\n",
    "browser.find_element_by_id(id)\n",
    "browser.find_elements_by_id(id)\n",
    "    Elements with a matching id attribute value\n",
    "\n",
    "browser.find_element_by_link_text(text)\n",
    "browser.find_elements_by_link_text(text)\n",
    "    <a> elements that completely match the text provided\n",
    "\n",
    "browser.find_element_by_partial_link_text(text)\n",
    "browser.find_elements_by_partial_link_text(text)\n",
    "    <a> elements that contain the text provided\n",
    "\n",
    "browser.find_element_by_name(name)\n",
    "browser.find_elements_by_name(name)\n",
    "    Elements with a matching name attribute value\n",
    "\n",
    "browser.find_element_by_tag_name(name)\n",
    "browser.find_elements_by_tag_name(name)\n",
    "    Elements with a matching tag name(case insensitive; an <a> element ismatched by 'a' and 'A')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tag_name \n",
    "    The tag name, such as 'a' for an <a> element\n",
    "get_attribute(name) \n",
    "    The value for the element’s name attribute \n",
    "text \n",
    "    The text within the element, such as 'hello' in <span>hello</span>\n",
    "clear() \n",
    "    For text field or text area elements, clears the text typed into it\n",
    "is_displayed() \n",
    "    Returns True if the element is visible; otherwise returns False\n",
    "is_enabled() \n",
    "    For input elements, returns True if the element is enabled; otherwise returns False\n",
    "is_selected() \n",
    "    For checkbox or radio button elements, returns True if the element is selected; otherwise returns False\n",
    "location \n",
    "    A dictionary with keys 'x' and 'y' for the position of the element in the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <img> element with that class name!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://inventwithpython.com')\n",
    "\n",
    "try:\n",
    "    #尋找 Elements that use the CSS class name\n",
    "    elem = browser.find_element_by_class_name('bookcover') \n",
    "    print('Found <%s> element with that class name!' % (elem.tag_name))\n",
    "except:\n",
    "    print('Was not able to find an element with that name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "帶目標tag的變數.click() 點按鍵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://inventwithpython.com')\n",
    "\n",
    "#尋找<a> elements that completely match the text provided\n",
    "linkElem = browser.find_element_by_link_text('Read It Online') \n",
    "type(linkElem)\n",
    "\n",
    "linkElem.click() # follows the \"Read It Online\" link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Out and Submitting Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driver.implicitly_wait(10) 讓網站休息等待目標跳出來!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://gmail.com')\n",
    "\n",
    "emailElem = browser.find_element_by_id('identifierId')\n",
    "emailElem.send_keys('@gmail.com')\n",
    "linkElem = browser.find_element_by_id('identifierNext')\n",
    "linkElem.click() #給我努力找到那個可以click的動作類別!!\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(10) # seconds\n",
    "#換頁面等10秒\n",
    "\n",
    "passwordElem = browser.find_element_by_id('password')\n",
    "passwordElem.send_keys('')\n",
    "linkElem = browser.find_element_by_id('passwordNext')\n",
    "linkElem.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending Special Keys"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Keys.DOWN, Keys.UP, Keys.LEFT, Keys.RIGHT                The keyboard arrow keys\n",
    "Keys.ENTER, Keys.RETURN                                  The enter and return keys\n",
    "Keys.HOME, Keys.END, Keys.PAGE_DOWN, Keys.PAGE_UP        The home, end, pagedown, and pageup keys\n",
    "Keys.ESCAPE, Keys.BACK_SPACE, Keys.DELETE                The esc, backspace, and delete keys\n",
    "Keys.F1, Keys.F2, . . . , Keys.F12                       The F1 to F12 keys at the top of the keyboard\n",
    "Keys.TAB                                                 The tab key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://nostarch.com')\n",
    "browser.implicitly_wait(10) # seconds\n",
    "#換頁面等10秒\n",
    "\n",
    "htmlElem = browser.find_element_by_tag_name('body')\n",
    "#簡單對整個html做事情，但是為什麼html標籤不可以呢?\n",
    "\n",
    "htmlElem.send_keys(Keys.END) # scrolls to bottom\n",
    "htmlElem.send_keys(Keys.HOME) # scrolls to top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking Browser Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser.back()     Clicks the Back button.\n",
    "browser.forward()  Clicks the Forward button.\n",
    "browser.refresh()  Clicks the Refresh/Reload button.\n",
    "browser.quit()     Clicks the Close Window button."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
